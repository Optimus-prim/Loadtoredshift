{
	"jobConfig": {
		"name": "case1_S2R",
		"description": "",
		"role": "arn:aws:iam::390402548407:role/service-role/AWSGlueServiceRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 15,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "case1_S2R.py",
		"scriptLocation": "s3://aws-glue-assets-390402548407-us-east-2/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--IAM_ROLE",
				"value": "arn:aws:iam::390402548407:role/RedshiftS3Role",
				"existing": false
			},
			{
				"key": "--REDSHIFT_CLUSTER_ID",
				"value": "redshift-cluster-3",
				"existing": false
			},
			{
				"key": "--REDSHIFT_DATABASE",
				"value": "dev",
				"existing": false
			},
			{
				"key": "--REDSHIFT_DB_USER",
				"value": "jajji",
				"existing": false
			},
			{
				"key": "--REGION",
				"value": "us-east-2",
				"existing": false
			},
			{
				"key": "--TABLE_NAME",
				"value": "sales_data",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-03-18T15:08:40.714Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-390402548407-us-east-2/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-390402548407-us-east-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\nimport boto3\nimport logging\nfrom awsglue.utils import getResolvedOptions\nimport json\n\n# Logging Configuration\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\nlogger = logging.getLogger(__name__)\n\n# Extract Arguments\nargs = getResolvedOptions(\n    sys.argv,\n    [\n        'input_bucket',\n        'input_key',\n        'REDSHIFT_CLUSTER_ID',\n        'REDSHIFT_DATABASE',\n        'REDSHIFT_DB_USER',\n        'IAM_ROLE',\n        'TABLE_NAME',\n        'REGION'\n    ]\n)\n\ninput_bucket = args['input_bucket']\ninput_key = args['input_key']\nREDSHIFT_CLUSTER_ID = args['REDSHIFT_CLUSTER_ID']\nREDSHIFT_DATABASE = args['REDSHIFT_DATABASE']\nREDSHIFT_DB_USER = args['REDSHIFT_DB_USER']\nIAM_ROLE = args['IAM_ROLE']\nTABLE_NAME = args['TABLE_NAME']\nREGION = args['REGION']\n\ndef main():\n    # Initialize Redshift Data API client\n    redshift_data = boto3.client('redshift-data', region_name=REGION)\n\n    # OPTIONAL: Create table if it doesn't exist\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n        order_id VARCHAR(50),\n        product_name VARCHAR(100),\n        category VARCHAR(50),\n        price FLOAT,\n        order_date DATE,\n        source VARCHAR(20),\n        price_category VARCHAR(20)\n    );\n    \"\"\"\n\n    try:\n        logger.info(f\"Creating table if not exists: {TABLE_NAME}\")\n        resp_create = redshift_data.execute_statement(\n            ClusterIdentifier=REDSHIFT_CLUSTER_ID,\n            Database=REDSHIFT_DATABASE,\n            DbUser=REDSHIFT_DB_USER,\n            Sql=create_table_sql\n        )\n        create_statement_id = resp_create['Id']\n        logger.info(f\"Create Table Statement ID: {create_statement_id}\")\n    except Exception as e:\n        logger.error(f\"Error creating table {TABLE_NAME}: {e}\", exc_info=True)\n        sys.exit(1)\n\n    # S3 path\n    s3_path = f\"s3://{input_bucket}/{input_key}\"\n\n    # COPY command to load CSV from S3\n    copy_sql = f\"\"\"\n    COPY {TABLE_NAME}\n    FROM '{s3_path}'\n    IAM_ROLE '{IAM_ROLE}'\n    FORMAT AS CSV\n    DELIMITER ','\n    IGNOREHEADER 1\n    REGION '{REGION}'\n    DATEFORMAT 'YYYY/MM/DD';\n    \"\"\"\n\n    try:\n        logger.info(f\"Running COPY command for table {TABLE_NAME}...\")\n        resp_copy = redshift_data.execute_statement(\n            ClusterIdentifier=REDSHIFT_CLUSTER_ID,\n            Database=REDSHIFT_DATABASE,\n            DbUser=REDSHIFT_DB_USER,\n            Sql=copy_sql\n        )\n        copy_statement_id = resp_copy['Id']\n        logger.info(f\"COPY Statement ID: {copy_statement_id}\")\n        logger.info(\"Data load request successfully submitted to Redshift.\")\n    except Exception as e:\n        logger.error(f\"Error loading data into {TABLE_NAME}: {e}\", exc_info=True)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except Exception as e:\n        logger.critical(f\"Pipeline failed due to unexpected error: {e}\", exc_info=True)\n        sys.exit(1)\n"
}